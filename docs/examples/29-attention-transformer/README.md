# Attention & Transformer Layers

Demonstrates MultiheadAttention and TransformerEncoderLayer for sequence-to-sequence modeling.

## Deepbox Modules Used

| Module            | Features Used                               |
| ----------------- | ------------------------------------------- |
| `deepbox/ndarray` | tensor, GradTensor                          |
| `deepbox/nn`      | MultiheadAttention, TransformerEncoderLayer |

## Usage

```bash
npm run example:29
```

## Output

- Console output showing self-attention and transformer encoder operations
